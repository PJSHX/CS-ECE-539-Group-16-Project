{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (4.66.1)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp38-none-win_amd64.whl (277 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp38-none-win_amd64.whl (2.2 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (23.2)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-win_amd64.whl (157 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->transformers) (2.1.0)\n",
      "Installing collected packages: pyyaml, huggingface-hub, tokenizers, safetensors, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 pyyaml-6.0.1 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\giris\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (0.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from sklearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->sklearn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->sklearn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->sklearn) (1.10.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->sklearn) (1.24.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\giris\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def text_summarizer(text, num_sentences=3):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # All tokens belong to a single segment in this case\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Get BERT embeddings for the input text\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        encoded_layers = outputs[0]\n",
    "\n",
    "    # Calculate sentence embeddings by averaging token embeddings for each sentence\n",
    "    token_embeddings = np.array(encoded_layers[0])\n",
    "    sentence_embeddings = []\n",
    "    current_sentence = []\n",
    "    for token in token_embeddings:\n",
    "        if len(current_sentence) < 512:  # Max token limit per sentence for BERT\n",
    "            current_sentence.append(token)\n",
    "        else:\n",
    "            sentence_embeddings.append(np.mean(current_sentence, axis=0))\n",
    "            current_sentence = [token]\n",
    "\n",
    "    if current_sentence:\n",
    "        sentence_embeddings.append(np.mean(current_sentence, axis=0))\n",
    "\n",
    "    # Calculate similarities between sentence embeddings\n",
    "    similarity_matrix = cosine_similarity(\n",
    "        sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "    # Create a similarity ranking for sentences\n",
    "    sentence_similarity_graph = similarity_matrix.argsort(axis=1)\n",
    "    sentence_similarity_graph = np.fliplr(sentence_similarity_graph)\n",
    "\n",
    "    # Generate summary based on the top-ranked sentences\n",
    "    summary = ''\n",
    "    sentence_count = 0\n",
    "    for row in sentence_similarity_graph:\n",
    "        if sentence_count < num_sentences:\n",
    "            summary += ' '.join(tokenized_text[row[0]]) + ' '\n",
    "            sentence_count += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 5.58kB/s]\n",
      "C:\\Users\\giris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\giris\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.86MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.84MB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 142kB/s]\n",
      "model.safetensors: 100%|██████████| 440M/440M [04:29<00:00, 1.63MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "Natural language processing (NLP) is a field of artificial intelligence \n",
      "(AI) that focuses on enabling computers to understand, interpret, and \n",
      "generate human language content. NLP techniques are used for various tasks \n",
      "such as language translation, sentiment analysis, text summarization, and \n",
      "more. One popular approach to NLP involves using pre-trained models like \n",
      "BERT, which is a powerful transformer-based model developed by Google. BERT \n",
      "has been widely used in natural language understanding and can be adapted \n",
      "for different NLP tasks through fine-tuning or transfer learning.\n",
      "\n",
      "\n",
      "Generated Summary:\n",
      " n a t u r a l\n"
     ]
    }
   ],
   "source": [
    "# Define a sample text for summarization\n",
    "sample_text = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence \n",
    "(AI) that focuses on enabling computers to understand, interpret, and \n",
    "generate human language content. NLP techniques are used for various tasks \n",
    "such as language translation, sentiment analysis, text summarization, and \n",
    "more. One popular approach to NLP involves using pre-trained models like \n",
    "BERT, which is a powerful transformer-based model developed by Google. BERT \n",
    "has been widely used in natural language understanding and can be adapted \n",
    "for different NLP tasks through fine-tuning or transfer learning.\n",
    "\"\"\"\n",
    "\n",
    "# Call the text summarizer function\n",
    "summary = text_summarizer(sample_text, num_sentences=2)\n",
    "\n",
    "# Print the original text and the generated summary\n",
    "print(\"Original Text:\\n\", sample_text)\n",
    "print(\"\\nGenerated Summary:\\n\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
