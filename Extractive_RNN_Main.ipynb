{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RX8_pChTTJum",
    "outputId": "8959e1d6-3454-41b2-9216-fa93ded807c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from rouge-score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk->rouge-score) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk->rouge-score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk->rouge-score) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Requirement already satisfied: evaluate in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (1.25.2)\n",
      "Requirement already satisfied: dill in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (2.1.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/1c/49/30ffcac5af06d08dfdd27da322ce31a373b733711bb272941877c1e4794a/scikit_learn-1.3.2-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.3 MB 991.0 kB/s eta 0:00:10\n",
      "    --------------------------------------- 0.2/9.3 MB 2.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/9.3 MB 4.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/9.3 MB 4.9 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/9.3 MB 4.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/9.3 MB 5.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.7/9.3 MB 5.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/9.3 MB 5.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.1/9.3 MB 5.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.8/9.3 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.1/9.3 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.5/9.3 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.3/9.3 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.6/9.3 MB 6.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.0/9.3 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.4/9.3 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.8/9.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.9/9.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.6/9.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.0/9.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.4/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.7/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.4/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.8/9.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n",
    "!pip install evaluate\n",
    "# Comment out below if on Colab\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5DDo_Ut5euFg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import evaluate\n",
    "#Reactivate if on Colab\n",
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5pwVHwWNezDn"
   },
   "outputs": [],
   "source": [
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JAKQxxqde1VS",
    "outputId": "1cfa8b93-a275-4350-f7c0-7b669f3b364b"
   },
   "outputs": [],
   "source": [
    "#Data Load\n",
    "#Change or switch as required by your setup\n",
    "#drive.mount('/content/drive')\n",
    "#data = pd.read_csv('drive/My Drive/CS_539/bbc-news-data.csv', delimiter='\\t')\n",
    "#data = pd.read_csv('drive/My Drive/COMP SCI 539/bbc-news-data.csv', delimiter='\\t')\n",
    "data = pd.read_csv('./bbc-news-data.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "3gPqivYze3Io",
    "outputId": "90527ce2-f0c6-4e09-81c3-882bb0f1c87f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>business</td>\n",
       "      <td>006.txt</td>\n",
       "      <td>Japan narrowly escapes recession</td>\n",
       "      <td>Japan's economy teetered on the brink of a te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>007.txt</td>\n",
       "      <td>Jobs growth still slow in the US</td>\n",
       "      <td>The US created fewer jobs than expected in Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>008.txt</td>\n",
       "      <td>India calls for fair trade rules</td>\n",
       "      <td>India, which attends the G7 meeting of seven ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>009.txt</td>\n",
       "      <td>Ethiopia's crop production up 24%</td>\n",
       "      <td>Ethiopia produced 14.27 million tonnes of cro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business</td>\n",
       "      <td>010.txt</td>\n",
       "      <td>Court rejects $280bn tobacco case</td>\n",
       "      <td>A US government claim accusing the country's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                              title  \\\n",
       "0  business  001.txt  Ad sales boost Time Warner profit   \n",
       "1  business  002.txt   Dollar gains on Greenspan speech   \n",
       "2  business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  High fuel prices hit BA's profits   \n",
       "4  business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "5  business  006.txt   Japan narrowly escapes recession   \n",
       "6  business  007.txt   Jobs growth still slow in the US   \n",
       "7  business  008.txt   India calls for fair trade rules   \n",
       "8  business  009.txt  Ethiopia's crop production up 24%   \n",
       "9  business  010.txt  Court rejects $280bn tobacco case   \n",
       "\n",
       "                                             content  \n",
       "0   Quarterly profits at US media giant TimeWarne...  \n",
       "1   The dollar has hit its highest level against ...  \n",
       "2   The owners of embattled Russian oil giant Yuk...  \n",
       "3   British Airways has blamed high fuel prices f...  \n",
       "4   Shares in UK drinks and food firm Allied Dome...  \n",
       "5   Japan's economy teetered on the brink of a te...  \n",
       "6   The US created fewer jobs than expected in Ja...  \n",
       "7   India, which attends the G7 meeting of seven ...  \n",
       "8   Ethiopia produced 14.27 million tonnes of cro...  \n",
       "9   A US government claim accusing the country's ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvdp0iuAe7T_",
    "outputId": "6872b701-9b37-4ca3-98bc-3c752e74b99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category    0\n",
      "filename    0\n",
      "title       0\n",
      "content     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "\n",
    "# Data to lowercase\n",
    "data[\"title\"] = data[\"title\"].str.lower()\n",
    "data[\"content\"] = data[\"content\"].str.lower()\n",
    "\n",
    "# List of contractions and acronyms to replace\n",
    "contraction_dict = {\"can't\":\"cannot\",\"didn't\":\"did not\",\"aren't\":\"are not\",\"she'd\":\"she would\",\"he'd\":\"he would\",\"they'd\":\"they would\",\"they've\":\"they have\",\n",
    "  \"shouldn't\":\"should not\",\"shouldn't've\":\"should not have\",\"she'll\":\"she will\",\"he'll\":\"he will\",\"they'll\":\"they will\", \"ba's\":\"british airways\",\n",
    "  \"g7\":\"group of seven\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "aB_HoAP5fHzR",
    "outputId": "27cf348d-d942-4517-fec4-234ccb29f81a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>ad sales boost time warner profit</td>\n",
       "      <td>quarterly profits at us media giant timewarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>dollar gains on greenspan speech</td>\n",
       "      <td>the dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>yukos unit buyer faces loan claim</td>\n",
       "      <td>the owners of embattled russian oil giant yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>high fuel prices hit british airways profits</td>\n",
       "      <td>british airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>pernod takeover talk lifts domecq</td>\n",
       "      <td>shares in uk drinks and food firm allied dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>business</td>\n",
       "      <td>006.txt</td>\n",
       "      <td>japan narrowly escapes recession</td>\n",
       "      <td>japan economy teetered on the brink of a tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>007.txt</td>\n",
       "      <td>jobs growth still slow in the us</td>\n",
       "      <td>the us created fewer jobs than expected in ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>008.txt</td>\n",
       "      <td>india calls for fair trade rules</td>\n",
       "      <td>india which attends the group of seven meetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>009.txt</td>\n",
       "      <td>ethiopia crop production up</td>\n",
       "      <td>ethiopia produced million tonnes of crops in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business</td>\n",
       "      <td>010.txt</td>\n",
       "      <td>court rejects bn tobacco case</td>\n",
       "      <td>a us government claim accusing the country bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                                         title  \\\n",
       "0  business  001.txt             ad sales boost time warner profit   \n",
       "1  business  002.txt              dollar gains on greenspan speech   \n",
       "2  business  003.txt             yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  high fuel prices hit british airways profits   \n",
       "4  business  005.txt             pernod takeover talk lifts domecq   \n",
       "5  business  006.txt              japan narrowly escapes recession   \n",
       "6  business  007.txt              jobs growth still slow in the us   \n",
       "7  business  008.txt              india calls for fair trade rules   \n",
       "8  business  009.txt                  ethiopia crop production up    \n",
       "9  business  010.txt                 court rejects bn tobacco case   \n",
       "\n",
       "                                             content  \n",
       "0   quarterly profits at us media giant timewarne...  \n",
       "1   the dollar has hit its highest level against ...  \n",
       "2   the owners of embattled russian oil giant yuk...  \n",
       "3   british airways has blamed high fuel prices f...  \n",
       "4   shares in uk drinks and food firm allied dome...  \n",
       "5   japan economy teetered on the brink of a tech...  \n",
       "6   the us created fewer jobs than expected in ja...  \n",
       "7   india which attends the group of seven meetin...  \n",
       "8   ethiopia produced million tonnes of crops in ...  \n",
       "9   a us government claim accusing the country bi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "def data_preprocessing(string, contraction_dict=contraction_dict):\n",
    "  for word, replace in contraction_dict.items():\n",
    "    string = string.replace(word, replace)\n",
    "  string = re.sub(r\"([.!?])\", r\" \\1\", string)\n",
    "  string = re.sub(r\"[^a-zA-Z!?]+\", r\" \", string)\n",
    "  string = re.sub(r\"\\b(s )\\b\", r\"\", string)\n",
    "  return string\n",
    "\n",
    "max_len_content = 0\n",
    "max_len_title = 0\n",
    "for index in range(len(data)):\n",
    "  data.loc[index,'title'] = data_preprocessing(data.loc[index,'title'])\n",
    "  if len(data.loc[index,'title'].split()) > max_len_title:\n",
    "    max_len_title = len(data.loc[index,'title'].split())\n",
    "  data.loc[index,'content'] = data_preprocessing(data.loc[index,'content'])\n",
    "  if len(data.loc[index,'content'].split()) > max_len_content:\n",
    "    max_len_content = len(data.loc[index,'content'].split())\n",
    "\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZs6HJVQg5zf",
    "outputId": "cf5d0b1d-d466-47ed-e134-1166db7f98c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 4453\n"
     ]
    }
   ],
   "source": [
    "print(max_len_title, max_len_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8Zdvf6xn-vp",
    "outputId": "8d02c7dd-2518-46ab-b2f1-4db31f68fc65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title vocab contains 3686 words.\n",
      "Content vocab contains 27771 words.\n"
     ]
    }
   ],
   "source": [
    "# Based on Transformer Tutorial\n",
    "class convert:\n",
    "  def __init__(self, category):\n",
    "    self.category = category #title or content\n",
    "    self.word_to_index = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"UNK\": 3}\n",
    "    self.index_to_word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "    self.word_to_count = {}\n",
    "    self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "\n",
    "  def add_sentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.add_word(word)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self.word_to_index:\n",
    "      self.word_to_index[word] = self.n_words\n",
    "      self.word_to_count[word] = 1\n",
    "      self.index_to_word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.word_to_count[word] += 1\n",
    "\n",
    "  def tokenize(self, sentence, seq_len=None):\n",
    "    tokens_indexed = [self.word_to_index[\"SOS\"]]\n",
    "\n",
    "    for tkn in sentence.split():\n",
    "      tokens_indexed.append(self.word_to_index[tkn if tkn in self.word_to_index else \"UNK\"])\n",
    "\n",
    "    tokens_indexed.append(self.word_to_index[\"EOS\"])\n",
    "\n",
    "    # Pad or trim to desired lengh\n",
    "    if seq_len is not None:\n",
    "      if len(tokens_indexed) < seq_len:\n",
    "        tokens_indexed += [self.word_to_index[\"PAD\"]] * (seq_len - len(tokens_indexed))\n",
    "      else:\n",
    "         tokens_indexed = tokens_indexed[:seq_len]\n",
    "\n",
    "    return tokens_indexed\n",
    "\n",
    "  def list_to_sentence(self, seq_ids):\n",
    "    return \" \".join([self.index_to_word[idx] for idx in seq_ids])\n",
    "\n",
    "\n",
    "title_vocab = convert(\"title\")\n",
    "content_vocab = convert(\"content\")\n",
    "\n",
    "for index in range(len(data)):\n",
    "  title_vocab.add_sentence(data.loc[index,'title'])\n",
    "  content_vocab.add_sentence(data.loc[index,'content'])\n",
    "\n",
    "print(f\"Title vocab contains {title_vocab.n_words} words.\")\n",
    "print(f\"Content vocab contains {content_vocab.n_words} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juQ3k6IFkWQm",
    "outputId": "dfa86020-929c-4afa-afab-3ce258852c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch | Content = torch.Size([32, 4453]) | title = torch.Size([32, 9])\n",
      "First sentence in contents:  SOS another front in the on going battle between microsoft and google is about to be opened by the end of microsoft aims to launch search software to find any kind of file on a pc hard drive the move is in answer to google release of its own search tool that catalogues data on desktop pcs the desktop search market is becoming increasingly crowded as google aol yahoo and many smaller firms tout programs that help people find files microsoft made the announcement about its forthcoming search software during a call to financial analysts to talk about its first quarter results john connors microsoft chief financial officer said a test version of its desktop search software should be available for download by the end of the year we re going to have a heck of a great race in search between google microsoft and yahoo he said it going to be really fun to follow microsoft is coming late to the desktop search arena and its software will have to compare favourably with programs from a large number of rivals many of which have fiercely dedicated populations of users the program could be based on the software microsoft owns as a result of its purchase of lookout software in early october on october google released desktop search software that catalogues all the files on a pc and lets users use one tool to find e mail messages spreadsheets text files and presentations the software will also find webpages and messages sent via aol instant messenger many other firms have released desktop search systems recently too companies such as blinkx copernic enfish x technologies and x friend all do the same job of cataloguing the huge amounts of information that people increasingly store on their desktop or home computer apple has also debuted a similar search system for its computers called spotlight that is due to debut with the release of the tiger operating system due to follow are net giants aol and yahoo the latter recently bought stata labs to get its hands on search software that people can use microsoft is also reputedly working on a novel search system for the next version of windows codenamed longhorn however this is not likely to appear until the recent activity in the search industry shows that there is a need to move beyond simple keyword based web search said kathy rittweger co founder of blinkx finding information of our own computers is becoming as difficult as it is to find the relevant webpage amongst the billions that exist desktop search has become important for several reasons according to research by message analysts the radicati group up to of the information critical to keeping many businesses running sits in e mail messages and attachments jf sullivan spokesman for e mail software firm sendmail said many organisations were starting to realise how important messaging was to their organisation and the way the work the key thing is being able to manage all this information he said also search is increasingly key to the way that people get around the internet many people use a search engine as the first page they go to when getting on the net many others use desktop toolbars that let them search for information no matter what other program they are using having a tool on a desktop can be a lucrative way to control where people go online for companies such as google which relies on revenue from adverts this knowledge about what people are looking for is worth huge amounts of money but this invasiveness has already led some to ask about the privacy implications of such tools EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "First sentence in titles: SOS search wars hit desktop pcs EOS PAD PAD\n"
     ]
    }
   ],
   "source": [
    "def data_loader(batch_size):\n",
    "  n = 2225\n",
    "  title_seqs_ids = torch.zeros((n, max_len_title)).long()\n",
    "  content_seqs_ids = torch.zeros((n, max_len_content)).long()\n",
    "\n",
    "  for index in range(len(data)):\n",
    "    title_seqs_ids[index] = torch.tensor(title_vocab.tokenize(data.loc[index,'title'],seq_len=max_len_title))\n",
    "    content_seqs_ids[index] = torch.tensor(content_vocab.tokenize(data.loc[index,'content'],seq_len=max_len_content))\n",
    "\n",
    "  #Train_test_split\n",
    "  X_train, X_test, y_train, y_test = train_test_split(content_seqs_ids,title_seqs_ids, train_size=0.6)\n",
    "\n",
    "\n",
    "  train_dataset = TensorDataset(X_train.to(device), y_train.to(device))\n",
    "  test_dataset = TensorDataset(X_test.to(device), y_test.to(device))\n",
    "\n",
    "  training_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "  return training_loader, test_dataset\n",
    "\n",
    "\n",
    "training_loader, test_data = data_loader(32)\n",
    "for x,y in training_loader:\n",
    "  print('Batch | Content =', x.shape, '| title =', y.shape)\n",
    "  print('First sentence in contents: ', content_vocab.list_to_sentence(x[0].tolist()))\n",
    "  print('First sentence in titles:', title_vocab.list_to_sentence(y[0].tolist()))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wEGMpGSyA3C",
    "outputId": "835d6820-52f5-410b-c026-0460ee5d2974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 382])\n",
      "torch.Size([1, 382, 512])\n"
     ]
    }
   ],
   "source": [
    "#Borrowed from Transformer Tutorial\n",
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "    [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "    axis=-1)\n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "class word_pos_embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "        self.pos_encoding = torch.Tensor(positional_encoding(length=2048, depth=d_model)).float().to(device)\n",
    "        self.pos_encoding.requires_grad = False\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        length = x.shape[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= (self.d_model ** 0.5)\n",
    "        x = x + self.pos_encoding[None, :length, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "embed_content = word_pos_embedding(vocab_size=content_vocab.n_words, d_model=512).to(device)\n",
    "embed_title = word_pos_embedding(vocab_size=title_vocab.n_words, d_model=512).to(device)\n",
    "\n",
    "#Testing Pos Embedding\n",
    "title_sen = data.loc[1,'title']\n",
    "title_seq = torch.tensor([title_vocab.word_to_index[w] for w in title_sen.split()]).unsqueeze(0)\n",
    "print(title_seq.shape)\n",
    "title_tkn_seq = embed_title(title_seq.to(device))\n",
    "print(title_tkn_seq.shape)\n",
    "\n",
    "content_sen = data.loc[1,'content']\n",
    "content_seq = torch.tensor([content_vocab.word_to_index[w] for w in content_sen.split()]).unsqueeze(0)\n",
    "print(content_seq.shape)\n",
    "content_tkn_seq = embed_content(content_seq.to(device))\n",
    "print(content_tkn_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nEFYkGIcDJoy"
   },
   "outputs": [],
   "source": [
    "#Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.embedding = word_pos_embedding(vocab_size, hidden_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        outputs, hidden = self.rnn(embed)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ep-81WeCDeLU"
   },
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.embedding = word_pos_embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, target, hidden):\n",
    "        x = self.embedding(target)\n",
    "        x = self.dropout(x)\n",
    "        x, (hidden, cell)= self.rnn(x, hidden)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t58446Efzq0X",
    "outputId": "24fc7264-81e2-4254-caa9-ef1f111c0fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of title Sentences: torch.Size([1, 5])\n",
      "Batch of content paragraphs: torch.Size([1, 382])\n",
      "Output of Causal Self-Attention: torch.Size([1, 5, 3686])\n"
     ]
    }
   ],
   "source": [
    "#Complete Model\n",
    "class base_rnn(nn.Module):\n",
    "    def __init__(self, hid_dim, embedding_dim, num_layers, input_vocab_size,\n",
    "                 target_vocab_size, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = num_layers\n",
    "        self.encoder = Encoder(input_vocab_size, embedding_dim, hid_dim,num_layers,dropout)\n",
    "        #self.layer = nn.RNN(hid_dim, hid_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = Decoder(target_vocab_size, embedding_dim, hid_dim, num_layers, dropout)\n",
    "        self.final_layer = nn.Linear(hid_dim, target_vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, source, target):\n",
    "      hidden = self.encoder(source)\n",
    "      output = self.decoder(target, hidden)\n",
    "      output = self.final_layer(output)\n",
    "      return(output)\n",
    "\n",
    "\n",
    "model = base_rnn( hid_dim = 512 ,embedding_dim = 512, num_layers=3,\n",
    "                 input_vocab_size = content_vocab.n_words,\n",
    "                 target_vocab_size= title_vocab.n_words,\n",
    "                  dropout = 0.1).to(device)\n",
    "\n",
    "print('Batch of title Sentences:', title_seq.shape)\n",
    "print('Batch of content paragraphs:', content_seq.shape)\n",
    "print('Output of Causal Self-Attention:', model(content_seq.to(device), title_seq.to(device)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JYObZLWQhzRb"
   },
   "outputs": [],
   "source": [
    "# Counting end of sequence and start of sequence\n",
    "title_seq_len = max_len_title + 2\n",
    "content_seq_len = max_len_content + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPinwuh-SXmn",
    "outputId": "285d0105-dba0-4d67-8a4f-b0abc543d9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for seq in training_loader:\n",
    "  print(type(seq))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hQawWe8QBHu7"
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "def train_sequence(seq, model, optimizer):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    for batch in range(len(seq[0])):\n",
    "      X = seq[0][batch]\n",
    "      y = seq[1][batch]\n",
    "\n",
    "      y_hat = model(X,y)\n",
    "      l = loss(y_hat, y.long())\n",
    "\n",
    "      total_loss += l.item()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      l.backward()\n",
    "      optimizer.step()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def fit(model, loader, lr, num_epochs=100):\n",
    "  optimizer = torch.optim.Adagrad(model.parameters(), lr)\n",
    "  for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for sequence in loader:\n",
    "      total_loss += train_sequence(sequence, model, optimizer)\n",
    "      total_loss /= len(loader)\n",
    "    print(f'Epoch {epoch} | Perplexity {np.exp(total_loss):.1f}. Loss: {total_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GZ_3IXKmT0TJ"
   },
   "outputs": [],
   "source": [
    "hid_dim = 64\n",
    "embed_dim = 64\n",
    "n_layer = 3\n",
    "batch_size = 20\n",
    "num_epochs = 100\n",
    "lr = 0.01\n",
    "model = base_rnn( hid_dim = hid_dim ,embedding_dim = embed_dim, num_layers=n_layer,\n",
    "                 input_vocab_size = content_vocab.n_words,\n",
    "                 target_vocab_size= title_vocab.n_words,\n",
    "                  dropout = 0.1).to(device)\n",
    "\n",
    "\n",
    "training_loader, test_data = data_loader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EryVaH62ZSTZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Perplexity 2.8. Loss: 1.017\n",
      "Epoch 1 | Perplexity 2.5. Loss: 0.927\n",
      "Epoch 2 | Perplexity 2.3. Loss: 0.836\n",
      "Epoch 3 | Perplexity 2.3. Loss: 0.839\n",
      "Epoch 4 | Perplexity 2.4. Loss: 0.872\n",
      "Epoch 5 | Perplexity 2.3. Loss: 0.843\n",
      "Epoch 6 | Perplexity 2.2. Loss: 0.800\n",
      "Epoch 7 | Perplexity 2.2. Loss: 0.802\n",
      "Epoch 8 | Perplexity 2.1. Loss: 0.756\n",
      "Epoch 9 | Perplexity 2.1. Loss: 0.731\n",
      "Epoch 10 | Perplexity 2.3. Loss: 0.825\n",
      "Epoch 11 | Perplexity 2.3. Loss: 0.833\n",
      "Epoch 12 | Perplexity 2.3. Loss: 0.821\n",
      "Epoch 13 | Perplexity 2.3. Loss: 0.822\n",
      "Epoch 14 | Perplexity 2.2. Loss: 0.779\n",
      "Epoch 15 | Perplexity 2.3. Loss: 0.816\n",
      "Epoch 16 | Perplexity 2.2. Loss: 0.790\n",
      "Epoch 17 | Perplexity 2.2. Loss: 0.801\n",
      "Epoch 18 | Perplexity 2.2. Loss: 0.780\n",
      "Epoch 19 | Perplexity 2.2. Loss: 0.789\n",
      "Epoch 20 | Perplexity 2.1. Loss: 0.742\n",
      "Epoch 21 | Perplexity 2.1. Loss: 0.737\n",
      "Epoch 22 | Perplexity 2.2. Loss: 0.787\n",
      "Epoch 23 | Perplexity 2.1. Loss: 0.764\n",
      "Epoch 24 | Perplexity 2.1. Loss: 0.759\n",
      "Epoch 25 | Perplexity 2.1. Loss: 0.724\n",
      "Epoch 26 | Perplexity 2.1. Loss: 0.757\n",
      "Epoch 27 | Perplexity 2.1. Loss: 0.755\n",
      "Epoch 28 | Perplexity 2.2. Loss: 0.773\n",
      "Epoch 29 | Perplexity 2.2. Loss: 0.786\n",
      "Epoch 30 | Perplexity 2.1. Loss: 0.758\n",
      "Epoch 31 | Perplexity 2.1. Loss: 0.744\n",
      "Epoch 32 | Perplexity 2.1. Loss: 0.730\n",
      "Epoch 33 | Perplexity 2.1. Loss: 0.721\n",
      "Epoch 34 | Perplexity 2.0. Loss: 0.703\n",
      "Epoch 35 | Perplexity 2.2. Loss: 0.776\n",
      "Epoch 36 | Perplexity 2.0. Loss: 0.691\n",
      "Epoch 37 | Perplexity 2.0. Loss: 0.710\n",
      "Epoch 38 | Perplexity 1.9. Loss: 0.657\n",
      "Epoch 39 | Perplexity 2.0. Loss: 0.716\n",
      "Epoch 40 | Perplexity 1.9. Loss: 0.634\n",
      "Epoch 41 | Perplexity 2.0. Loss: 0.706\n",
      "Epoch 42 | Perplexity 2.0. Loss: 0.677\n",
      "Epoch 43 | Perplexity 2.0. Loss: 0.669\n",
      "Epoch 44 | Perplexity 2.1. Loss: 0.731\n",
      "Epoch 45 | Perplexity 2.0. Loss: 0.671\n",
      "Epoch 46 | Perplexity 1.9. Loss: 0.629\n",
      "Epoch 47 | Perplexity 2.1. Loss: 0.729\n",
      "Epoch 48 | Perplexity 2.0. Loss: 0.695\n",
      "Epoch 49 | Perplexity 2.1. Loss: 0.748\n",
      "Epoch 50 | Perplexity 2.0. Loss: 0.670\n",
      "Epoch 51 | Perplexity 1.9. Loss: 0.643\n",
      "Epoch 52 | Perplexity 1.8. Loss: 0.611\n",
      "Epoch 53 | Perplexity 2.0. Loss: 0.691\n",
      "Epoch 54 | Perplexity 2.0. Loss: 0.713\n",
      "Epoch 55 | Perplexity 1.9. Loss: 0.656\n",
      "Epoch 56 | Perplexity 1.9. Loss: 0.626\n",
      "Epoch 57 | Perplexity 1.9. Loss: 0.621\n",
      "Epoch 58 | Perplexity 2.0. Loss: 0.668\n",
      "Epoch 59 | Perplexity 1.9. Loss: 0.662\n",
      "Epoch 60 | Perplexity 1.9. Loss: 0.617\n",
      "Epoch 61 | Perplexity 1.9. Loss: 0.664\n",
      "Epoch 62 | Perplexity 1.9. Loss: 0.639\n",
      "Epoch 63 | Perplexity 1.9. Loss: 0.666\n",
      "Epoch 64 | Perplexity 1.9. Loss: 0.634\n",
      "Epoch 65 | Perplexity 2.0. Loss: 0.669\n",
      "Epoch 66 | Perplexity 1.9. Loss: 0.618\n",
      "Epoch 67 | Perplexity 1.9. Loss: 0.635\n",
      "Epoch 68 | Perplexity 1.8. Loss: 0.582\n",
      "Epoch 69 | Perplexity 1.9. Loss: 0.661\n",
      "Epoch 70 | Perplexity 1.9. Loss: 0.656\n",
      "Epoch 71 | Perplexity 1.8. Loss: 0.586\n",
      "Epoch 72 | Perplexity 1.9. Loss: 0.632\n",
      "Epoch 73 | Perplexity 1.9. Loss: 0.651\n",
      "Epoch 74 | Perplexity 1.9. Loss: 0.625\n",
      "Epoch 75 | Perplexity 1.9. Loss: 0.624\n",
      "Epoch 76 | Perplexity 1.8. Loss: 0.578\n",
      "Epoch 77 | Perplexity 1.8. Loss: 0.584\n",
      "Epoch 78 | Perplexity 2.0. Loss: 0.673\n",
      "Epoch 79 | Perplexity 1.9. Loss: 0.625\n",
      "Epoch 80 | Perplexity 1.8. Loss: 0.611\n",
      "Epoch 81 | Perplexity 1.9. Loss: 0.643\n",
      "Epoch 82 | Perplexity 1.8. Loss: 0.610\n",
      "Epoch 83 | Perplexity 1.9. Loss: 0.634\n",
      "Epoch 84 | Perplexity 2.0. Loss: 0.694\n",
      "Epoch 85 | Perplexity 1.8. Loss: 0.599\n",
      "Epoch 86 | Perplexity 1.9. Loss: 0.638\n",
      "Epoch 87 | Perplexity 1.9. Loss: 0.618\n",
      "Epoch 88 | Perplexity 1.9. Loss: 0.635\n",
      "Epoch 89 | Perplexity 1.9. Loss: 0.622\n",
      "Epoch 90 | Perplexity 1.8. Loss: 0.615\n",
      "Epoch 91 | Perplexity 1.8. Loss: 0.598\n",
      "Epoch 92 | Perplexity 1.7. Loss: 0.551\n",
      "Epoch 93 | Perplexity 1.9. Loss: 0.643\n",
      "Epoch 94 | Perplexity 1.8. Loss: 0.580\n",
      "Epoch 95 | Perplexity 1.8. Loss: 0.593\n",
      "Epoch 96 | Perplexity 1.8. Loss: 0.564\n",
      "Epoch 97 | Perplexity 1.7. Loss: 0.557\n",
      "Epoch 98 | Perplexity 1.8. Loss: 0.600\n",
      "Epoch 99 | Perplexity 1.8. Loss: 0.578\n"
     ]
    }
   ],
   "source": [
    "fit(model, training_loader, lr, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oizbCrERZSSW"
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QrN4bdmlT0SQ"
   },
   "outputs": [],
   "source": [
    "#Rouge Testing\n",
    "rouge = evaluate.load('rouge')\n",
    "#y_pred = \n",
    "#results = rouge.compute(predictions=, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
