{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RX8_pChTTJum",
    "outputId": "8959e1d6-3454-41b2-9216-fa93ded807c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from rouge-score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk->rouge-score) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk->rouge-score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk->rouge-score) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Requirement already satisfied: evaluate in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (1.25.2)\n",
      "Requirement already satisfied: dill in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (2.1.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n",
    "!pip install evaluate\n",
    "# Comment out below if on Colab\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5DDo_Ut5euFg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjsha\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import evaluate\n",
    "#Reactivate if on Colab\n",
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5pwVHwWNezDn"
   },
   "outputs": [],
   "source": [
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JAKQxxqde1VS",
    "outputId": "1cfa8b93-a275-4350-f7c0-7b669f3b364b"
   },
   "outputs": [],
   "source": [
    "#Data Load\n",
    "#Change or switch as required by your setup\n",
    "#drive.mount('/content/drive')\n",
    "#data = pd.read_csv('drive/My Drive/CS_539/bbc-news-data.csv', delimiter='\\t')\n",
    "#data = pd.read_csv('drive/My Drive/COMP SCI 539/bbc-news-data.csv', delimiter='\\t')\n",
    "data = pd.read_csv('./bbc-news-data.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "3gPqivYze3Io",
    "outputId": "90527ce2-f0c6-4e09-81c3-882bb0f1c87f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>business</td>\n",
       "      <td>006.txt</td>\n",
       "      <td>Japan narrowly escapes recession</td>\n",
       "      <td>Japan's economy teetered on the brink of a te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>007.txt</td>\n",
       "      <td>Jobs growth still slow in the US</td>\n",
       "      <td>The US created fewer jobs than expected in Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>008.txt</td>\n",
       "      <td>India calls for fair trade rules</td>\n",
       "      <td>India, which attends the G7 meeting of seven ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>009.txt</td>\n",
       "      <td>Ethiopia's crop production up 24%</td>\n",
       "      <td>Ethiopia produced 14.27 million tonnes of cro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business</td>\n",
       "      <td>010.txt</td>\n",
       "      <td>Court rejects $280bn tobacco case</td>\n",
       "      <td>A US government claim accusing the country's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                              title  \\\n",
       "0  business  001.txt  Ad sales boost Time Warner profit   \n",
       "1  business  002.txt   Dollar gains on Greenspan speech   \n",
       "2  business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  High fuel prices hit BA's profits   \n",
       "4  business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "5  business  006.txt   Japan narrowly escapes recession   \n",
       "6  business  007.txt   Jobs growth still slow in the US   \n",
       "7  business  008.txt   India calls for fair trade rules   \n",
       "8  business  009.txt  Ethiopia's crop production up 24%   \n",
       "9  business  010.txt  Court rejects $280bn tobacco case   \n",
       "\n",
       "                                             content  \n",
       "0   Quarterly profits at US media giant TimeWarne...  \n",
       "1   The dollar has hit its highest level against ...  \n",
       "2   The owners of embattled Russian oil giant Yuk...  \n",
       "3   British Airways has blamed high fuel prices f...  \n",
       "4   Shares in UK drinks and food firm Allied Dome...  \n",
       "5   Japan's economy teetered on the brink of a te...  \n",
       "6   The US created fewer jobs than expected in Ja...  \n",
       "7   India, which attends the G7 meeting of seven ...  \n",
       "8   Ethiopia produced 14.27 million tonnes of cro...  \n",
       "9   A US government claim accusing the country's ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvdp0iuAe7T_",
    "outputId": "6872b701-9b37-4ca3-98bc-3c752e74b99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category    0\n",
      "filename    0\n",
      "title       0\n",
      "content     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "\n",
    "# Data to lowercase\n",
    "data[\"title\"] = data[\"title\"].str.lower()\n",
    "data[\"content\"] = data[\"content\"].str.lower()\n",
    "\n",
    "# List of contractions and acronyms to replace\n",
    "contraction_dict = {\"can't\":\"cannot\",\"didn't\":\"did not\",\"aren't\":\"are not\",\"she'd\":\"she would\",\"he'd\":\"he would\",\"they'd\":\"they would\",\"they've\":\"they have\",\n",
    "  \"shouldn't\":\"should not\",\"shouldn't've\":\"should not have\",\"she'll\":\"she will\",\"he'll\":\"he will\",\"they'll\":\"they will\", \"ba's\":\"british airways\",\n",
    "  \"g7\":\"group of seven\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "aB_HoAP5fHzR",
    "outputId": "27cf348d-d942-4517-fec4-234ccb29f81a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>ad sales boost time warner profit</td>\n",
       "      <td>quarterly profits at us media giant timewarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>dollar gains on greenspan speech</td>\n",
       "      <td>the dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>yukos unit buyer faces loan claim</td>\n",
       "      <td>the owners of embattled russian oil giant yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>high fuel prices hit british airways profits</td>\n",
       "      <td>british airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>pernod takeover talk lifts domecq</td>\n",
       "      <td>shares in uk drinks and food firm allied dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>business</td>\n",
       "      <td>006.txt</td>\n",
       "      <td>japan narrowly escapes recession</td>\n",
       "      <td>japan economy teetered on the brink of a tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>007.txt</td>\n",
       "      <td>jobs growth still slow in the us</td>\n",
       "      <td>the us created fewer jobs than expected in ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>008.txt</td>\n",
       "      <td>india calls for fair trade rules</td>\n",
       "      <td>india which attends the group of seven meetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>009.txt</td>\n",
       "      <td>ethiopia crop production up</td>\n",
       "      <td>ethiopia produced million tonnes of crops in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business</td>\n",
       "      <td>010.txt</td>\n",
       "      <td>court rejects bn tobacco case</td>\n",
       "      <td>a us government claim accusing the country bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                                         title  \\\n",
       "0  business  001.txt             ad sales boost time warner profit   \n",
       "1  business  002.txt              dollar gains on greenspan speech   \n",
       "2  business  003.txt             yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  high fuel prices hit british airways profits   \n",
       "4  business  005.txt             pernod takeover talk lifts domecq   \n",
       "5  business  006.txt              japan narrowly escapes recession   \n",
       "6  business  007.txt              jobs growth still slow in the us   \n",
       "7  business  008.txt              india calls for fair trade rules   \n",
       "8  business  009.txt                  ethiopia crop production up    \n",
       "9  business  010.txt                 court rejects bn tobacco case   \n",
       "\n",
       "                                             content  \n",
       "0   quarterly profits at us media giant timewarne...  \n",
       "1   the dollar has hit its highest level against ...  \n",
       "2   the owners of embattled russian oil giant yuk...  \n",
       "3   british airways has blamed high fuel prices f...  \n",
       "4   shares in uk drinks and food firm allied dome...  \n",
       "5   japan economy teetered on the brink of a tech...  \n",
       "6   the us created fewer jobs than expected in ja...  \n",
       "7   india which attends the group of seven meetin...  \n",
       "8   ethiopia produced million tonnes of crops in ...  \n",
       "9   a us government claim accusing the country bi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "def data_preprocessing(string, contraction_dict=contraction_dict):\n",
    "  for word, replace in contraction_dict.items():\n",
    "    string = string.replace(word, replace)\n",
    "  string = re.sub(r\"([.!?])\", r\" \\1\", string)\n",
    "  string = re.sub(r\"[^a-zA-Z!?]+\", r\" \", string)\n",
    "  string = re.sub(r\"\\b(s )\\b\", r\"\", string)\n",
    "  return string\n",
    "\n",
    "max_len_content = 0\n",
    "max_len_title = 0\n",
    "for index in range(len(data)):\n",
    "  data.loc[index,'title'] = data_preprocessing(data.loc[index,'title'])\n",
    "  if len(data.loc[index,'title'].split()) > max_len_title:\n",
    "    max_len_title = len(data.loc[index,'title'].split())\n",
    "  data.loc[index,'content'] = data_preprocessing(data.loc[index,'content'])\n",
    "  if len(data.loc[index,'content'].split()) > max_len_content:\n",
    "    max_len_content = len(data.loc[index,'content'].split())\n",
    "\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZs6HJVQg5zf",
    "outputId": "cf5d0b1d-d466-47ed-e134-1166db7f98c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 4453\n"
     ]
    }
   ],
   "source": [
    "print(max_len_title, max_len_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8Zdvf6xn-vp",
    "outputId": "8d02c7dd-2518-46ab-b2f1-4db31f68fc65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title vocab contains 3686 words.\n",
      "Content vocab contains 27771 words.\n"
     ]
    }
   ],
   "source": [
    "# Based on Transformer Tutorial\n",
    "class convert:\n",
    "  def __init__(self, category):\n",
    "    self.category = category #title or content\n",
    "    self.word_to_index = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"UNK\": 3}\n",
    "    self.index_to_word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "    self.word_to_count = {}\n",
    "    self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "\n",
    "  def add_sentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.add_word(word)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self.word_to_index:\n",
    "      self.word_to_index[word] = self.n_words\n",
    "      self.word_to_count[word] = 1\n",
    "      self.index_to_word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.word_to_count[word] += 1\n",
    "\n",
    "  def tokenize(self, sentence, seq_len=None):\n",
    "    tokens_indexed = [self.word_to_index[\"SOS\"]]\n",
    "\n",
    "    for tkn in sentence.split():\n",
    "      tokens_indexed.append(self.word_to_index[tkn if tkn in self.word_to_index else \"UNK\"])\n",
    "\n",
    "    tokens_indexed.append(self.word_to_index[\"EOS\"])\n",
    "\n",
    "    # Pad or trim to desired lengh\n",
    "    if seq_len is not None:\n",
    "      if len(tokens_indexed) < seq_len:\n",
    "        tokens_indexed += [self.word_to_index[\"PAD\"]] * (seq_len - len(tokens_indexed))\n",
    "      else:\n",
    "         tokens_indexed = tokens_indexed[:seq_len]\n",
    "\n",
    "    return tokens_indexed\n",
    "\n",
    "  def list_to_sentence(self, seq_ids):\n",
    "    return \" \".join([self.index_to_word[idx] for idx in seq_ids])\n",
    "\n",
    "\n",
    "title_vocab = convert(\"title\")\n",
    "content_vocab = convert(\"content\")\n",
    "\n",
    "for index in range(len(data)):\n",
    "  title_vocab.add_sentence(data.loc[index,'title'])\n",
    "  content_vocab.add_sentence(data.loc[index,'content'])\n",
    "\n",
    "print(f\"Title vocab contains {title_vocab.n_words} words.\")\n",
    "print(f\"Content vocab contains {content_vocab.n_words} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juQ3k6IFkWQm",
    "outputId": "dfa86020-929c-4afa-afab-3ce258852c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch | Content = torch.Size([32, 4453]) | title = torch.Size([32, 9])\n",
      "First sentence in contents:  SOS goals from gregory vignal and nacho novo gave rangers a scrappy victory at celtic park that moves them three points clear of the champions rangers had rarely threatened until celtic goalkeeper rab douglas let defender vignal yard drive slip through his grasp and into the net opposite number ronald waterreus had been rangers hero saving superbly from craig bellamy and john hartson striker novo secured victory lobbing douglas with eight minutes remaining it ended celtic game unbeaten run at home in old firm derbies and gave rangers manager alex mcleish his first victory at the home of his glasgow rivals celtic had won their last six meetings on their home pitch including twice already this season they started confidently with new signing bellamy on loan from newcastle united given his celtic debut up front with wales international colleague john hartson and chris sutton dropping into midfield it took bellamy just four minutes to threaten taking on marvin andrews before delivering a low drive that was held by waterreus at the second attempt he had an even better chance after hartson dispossesed sotiris kyrgiakos and sent his strike partner clear with only the goalkeeper to beat but waterreus did well to beat away bellamy disappointing low drive from yards waterreus came to the rescue again when the ball fell to hartson just inside the box and the dutch goalkeeper made a brave block it was an old firm return for barry ferguson as mcleish stuck by the side that thumped four goals past hibernian but rangers found celtic harder to break down and douglas was not threatened until minutes after the break dado prso turned inside neil lennon only for the celtic goalkeeper to beat away his powerful yard drive a great defensive header by andrews prevented hartson pouncing from five yards out hartson foxed vignal at the edge of the rangers box but the striker shot on the turn was again beaten away by waterreus rangers were beginning to dominate the midfield and vignal collecting a knock back from fernando ricksen broke the deadlock douglas somehow letting the frenchman dipping drive slip through his grasp novo pounced on a moments hesitation in the celtic defence to latch on to a long ball from ricksen and lob the ball over the advancing douglas ricksen appeared to be hit by a coin but it could not prevent rangers celebrations at the final whistle douglas mcnamara balde varga laursen petrov lennon sutton thompson bellamy hartson subs marshall henchoz juninho paulista lambert maloney wallace mcgeady waterreus hutton kyrgiakos andrews ball buffel ferguson ricksen vignal prso novo subs mcgregor namouchi burke alex rae malcolm thompson lovenkrands m mccurry EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "First sentence in titles: SOS rangers seal old firm win EOS PAD PAD\n"
     ]
    }
   ],
   "source": [
    "def data_loader(batch_size):\n",
    "  n = 2225\n",
    "  title_seqs_ids = torch.zeros((n, max_len_title)).long()\n",
    "  content_seqs_ids = torch.zeros((n, max_len_content)).long()\n",
    "\n",
    "  for index in range(len(data)):\n",
    "    title_seqs_ids[index] = torch.tensor(title_vocab.tokenize(data.loc[index,'title'],seq_len=max_len_title))\n",
    "    content_seqs_ids[index] = torch.tensor(content_vocab.tokenize(data.loc[index,'content'],seq_len=max_len_content))\n",
    "\n",
    "  #Train_test_split\n",
    "  X_train, X_test, y_train, y_test = train_test_split(content_seqs_ids,title_seqs_ids, train_size=0.6)\n",
    "\n",
    "\n",
    "  train_dataset = TensorDataset(X_train.to(device), y_train.to(device))\n",
    "  test_dataset = TensorDataset(X_test.to(device), y_test.to(device))\n",
    "\n",
    "  training_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "  return training_loader, test_dataset\n",
    "\n",
    "\n",
    "training_loader, test_data = data_loader(32)\n",
    "for x,y in training_loader:\n",
    "  print('Batch | Content =', x.shape, '| title =', y.shape)\n",
    "  print('First sentence in contents: ', content_vocab.list_to_sentence(x[0].tolist()))\n",
    "  print('First sentence in titles:', title_vocab.list_to_sentence(y[0].tolist()))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wEGMpGSyA3C",
    "outputId": "835d6820-52f5-410b-c026-0460ee5d2974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 382])\n",
      "torch.Size([1, 382, 512])\n"
     ]
    }
   ],
   "source": [
    "#Borrowed from Transformer Tutorial\n",
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "    [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "    axis=-1)\n",
    "\n",
    "  return pos_encoding\n",
    "\n",
    "class word_pos_embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "        self.pos_encoding = torch.Tensor(positional_encoding(length=2048, depth=d_model)).float().to(device)\n",
    "        self.pos_encoding.requires_grad = False\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        length = x.shape[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= (self.d_model ** 0.5)\n",
    "        x = x + self.pos_encoding[None, :length, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "embed_content = word_pos_embedding(vocab_size=content_vocab.n_words, d_model=512).to(device)\n",
    "embed_title = word_pos_embedding(vocab_size=title_vocab.n_words, d_model=512).to(device)\n",
    "\n",
    "#Testing Pos Embedding\n",
    "title_sen = data.loc[1,'title']\n",
    "title_seq = torch.tensor([title_vocab.word_to_index[w] for w in title_sen.split()]).unsqueeze(0)\n",
    "print(title_seq.shape)\n",
    "title_tkn_seq = embed_title(title_seq.to(device))\n",
    "print(title_tkn_seq.shape)\n",
    "\n",
    "content_sen = data.loc[1,'content']\n",
    "content_seq = torch.tensor([content_vocab.word_to_index[w] for w in content_sen.split()]).unsqueeze(0)\n",
    "print(content_seq.shape)\n",
    "content_tkn_seq = embed_content(content_seq.to(device))\n",
    "print(content_tkn_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nEFYkGIcDJoy"
   },
   "outputs": [],
   "source": [
    "#Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.embedding = word_pos_embedding(vocab_size, hidden_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        outputs, hidden = self.rnn(embed)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ep-81WeCDeLU"
   },
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.embedding = word_pos_embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        #self.rnn = nn.LSTM(hidden_dim, vocab_size, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, target, hidden):\n",
    "        x = self.embedding(target)\n",
    "        x = self.dropout(x)\n",
    "        x, (hidden, cell)= self.rnn(x, hidden)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t58446Efzq0X",
    "outputId": "24fc7264-81e2-4254-caa9-ef1f111c0fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of title Sentences: torch.Size([1, 5])\n",
      "Batch of content paragraphs: torch.Size([1, 382])\n",
      "Output of Causal Self-Attention: torch.Size([1, 5, 3686])\n"
     ]
    }
   ],
   "source": [
    "#Complete Model\n",
    "class base_rnn(nn.Module):\n",
    "    def __init__(self, hid_dim, embedding_dim, num_layers, input_vocab_size,\n",
    "                 target_vocab_size, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = num_layers\n",
    "        self.encoder = Encoder(input_vocab_size, embedding_dim, hid_dim,num_layers, dropout)\n",
    "        #self.layer = nn.RNN(hid_dim, hid_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = Decoder(target_vocab_size, embedding_dim, hid_dim, num_layers, dropout)\n",
    "        self.final_layer = nn.Linear(hid_dim, target_vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, source, target):\n",
    "      hidden = self.encoder(source)\n",
    "      output = self.decoder(target, hidden)\n",
    "      output = self.final_layer(output)\n",
    "      return(output)\n",
    "\n",
    "\n",
    "model = base_rnn( hid_dim = 512 ,embedding_dim = 512, num_layers=3,\n",
    "                 input_vocab_size = content_vocab.n_words,\n",
    "                 target_vocab_size= title_vocab.n_words,\n",
    "                  dropout = 0.1).to(device)\n",
    "\n",
    "print('Batch of title Sentences:', title_seq.shape)\n",
    "print('Batch of content paragraphs:', content_seq.shape)\n",
    "print('Output of Causal Self-Attention:', model(content_seq.to(device), title_seq.to(device)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JYObZLWQhzRb"
   },
   "outputs": [],
   "source": [
    "# Counting end of sequence and start of sequence\n",
    "title_seq_len = max_len_title + 2\n",
    "content_seq_len = max_len_content + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPinwuh-SXmn",
    "outputId": "285d0105-dba0-4d67-8a4f-b0abc543d9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for seq in training_loader:\n",
    "  print(type(seq))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hQawWe8QBHu7"
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "def train_sequence(seq, model, optimizer):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    for batch in range(len(seq[0])):\n",
    "      X = seq[0][batch]\n",
    "      y = seq[1][batch]\n",
    "\n",
    "      y_hat = model(X,y)\n",
    "      l = loss(y_hat, y.long())\n",
    "\n",
    "      total_loss += l.item()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      l.backward()\n",
    "      optimizer.step()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def fit(model, loader, lr, num_epochs=100):\n",
    "  optimizer = torch.optim.Adagrad(model.parameters(), lr)\n",
    "  for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for sequence in loader:\n",
    "      total_loss += train_sequence(sequence, model, optimizer)\n",
    "      total_loss /= len(loader)\n",
    "    print(f'Epoch {epoch} | Perplexity {np.exp(total_loss):.1f}. Loss: {total_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GZ_3IXKmT0TJ"
   },
   "outputs": [],
   "source": [
    "hid_dim = 96\n",
    "embed_dim = 96\n",
    "n_layer = 3\n",
    "batch_size = 25\n",
    "num_epochs = 20\n",
    "lr = 0.05\n",
    "model = base_rnn( hid_dim = hid_dim ,embedding_dim = embed_dim, num_layers=n_layer,\n",
    "                 input_vocab_size = content_vocab.n_words,\n",
    "                 target_vocab_size= title_vocab.n_words,\n",
    "                  dropout = 0.1).to(device)\n",
    "\n",
    "\n",
    "training_loader, test_data = data_loader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EryVaH62ZSTZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Perplexity 2.2. Loss: 0.789\n",
      "Epoch 1 | Perplexity 2.0. Loss: 0.715\n",
      "Epoch 2 | Perplexity 1.9. Loss: 0.616\n",
      "Epoch 3 | Perplexity 1.9. Loss: 0.627\n",
      "Epoch 4 | Perplexity 1.8. Loss: 0.566\n",
      "Epoch 5 | Perplexity 1.6. Loss: 0.489\n",
      "Epoch 6 | Perplexity 1.6. Loss: 0.457\n",
      "Epoch 7 | Perplexity 1.6. Loss: 0.457\n",
      "Epoch 8 | Perplexity 1.5. Loss: 0.394\n",
      "Epoch 9 | Perplexity 1.4. Loss: 0.366\n",
      "Epoch 10 | Perplexity 1.5. Loss: 0.387\n",
      "Epoch 11 | Perplexity 1.4. Loss: 0.339\n",
      "Epoch 12 | Perplexity 1.3. Loss: 0.297\n",
      "Epoch 13 | Perplexity 1.2. Loss: 0.210\n",
      "Epoch 14 | Perplexity 1.3. Loss: 0.261\n",
      "Epoch 15 | Perplexity 1.2. Loss: 0.158\n",
      "Epoch 16 | Perplexity 1.2. Loss: 0.204\n",
      "Epoch 17 | Perplexity 1.2. Loss: 0.159\n",
      "Epoch 18 | Perplexity 1.2. Loss: 0.165\n",
      "Epoch 19 | Perplexity 1.2. Loss: 0.159\n"
     ]
    }
   ],
   "source": [
    "fit(model, training_loader, lr, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oizbCrERZSSW"
   },
   "outputs": [],
   "source": [
    "#Rouge Testing\n",
    "def rouge_test(model, test_data):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for seq in test_data:\n",
    "        X = seq[0]\n",
    "        y = seq[1]\n",
    "        print(title_vocab.list_to_sentence(y.tolist()))\n",
    "        references.append(title_vocab.list_to_sentence(y.tolist()))\n",
    "        y_pred = model(X,y)\n",
    "        print(y_pred)\n",
    "        predictions.append(title_vocab.list_to_sentence(y_pred.tolist()))                    \n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QrN4bdmlT0SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS speak easy plan for media players EOS PAD\n",
      "tensor([[-14.1140,   1.5952,  -9.8449,  ..., -30.6725, -23.0170, -32.1366],\n",
      "        [-10.8363, -10.1807,  -9.3408,  ..., -17.0762, -10.8703, -15.2854],\n",
      "        [-16.1511, -13.8509, -15.1515,  ..., -13.6506, -14.0826,  -9.9297],\n",
      "        ...,\n",
      "        [-22.8354, -21.1582, -19.6010,  ..., -19.7693, -14.0335, -17.1399],\n",
      "        [-11.9852, -12.6117,   0.2747,  ..., -34.6375, -29.2423, -33.0126],\n",
      "        [ -0.9565, -16.3543, -12.8544,  ..., -38.2427, -28.2928, -36.7394]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrouge_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m, in \u001b[0;36mrouge_test\u001b[1;34m(model, test_data)\u001b[0m\n\u001b[0;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X,y)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y_pred)\n\u001b[1;32m---> 14\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtitle_vocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_to_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)                    \n\u001b[0;32m     15\u001b[0m results \u001b[38;5;241m=\u001b[39m rouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mreferences)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36mconvert.list_to_sentence\u001b[1;34m(self, seq_ids)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_to_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq_ids):\n\u001b[1;32m---> 42\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_word[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m seq_ids])\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_to_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq_ids):\n\u001b[1;32m---> 42\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_to_word\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m seq_ids])\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "rouge_test(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
