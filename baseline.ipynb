{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: click in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\giris\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from sklearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->sklearn) (3.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->sklearn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->sklearn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->sklearn) (1.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\giris\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.0.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\giris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\giris\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install sklearn\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                              title  \\\n",
       "0  business  001.txt  Ad sales boost Time Warner profit   \n",
       "1  business  002.txt   Dollar gains on Greenspan speech   \n",
       "2  business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  High fuel prices hit BA's profits   \n",
       "4  business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "\n",
       "                                             content  \n",
       "0   Quarterly profits at US media giant TimeWarne...  \n",
       "1   The dollar has hit its highest level against ...  \n",
       "2   The owners of embattled Russian oil giant Yuk...  \n",
       "3   British Airways has blamed high fuel prices f...  \n",
       "4   Shares in UK drinks and food firm Allied Dome...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load & check Data\n",
    "\n",
    "colab = False\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data = pd.read_csv(\n",
    "        'drive/My Drive/COMP SCI 539/bbc-news-data.csv', delimiter='\\t')\n",
    "else:\n",
    "    data = pd.read_csv('bbc-news-data.csv', delimiter='\\t')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category    0\n",
      "filename    0\n",
      "title       0\n",
      "content     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# Find and remove nulls\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Data to lowercase\n",
    "data[\"title\"] = data[\"title\"].str.lower()\n",
    "data[\"content\"] = data[\"content\"].str.lower()\n",
    "# Remove and replace contractions\n",
    "# Find more contraction in text and add\n",
    "contraction_dict = {\"can't\": \"cannot\", \"didn't\": \"did not\", \"aren't\": \"are not\", \"she'd\": \"she would\", \"he'd\": \"he would\", \"they'd\": \"they would\", \"they've\": \"they have\",\n",
    "                    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"she'll\": \"she will\", \"he'll\": \"he will\", \"they'll\": \"they will\"\n",
    "                    }\n",
    "\n",
    "\n",
    "def contraction_replacer(text):\n",
    "    for word in text.split():\n",
    "        if word in contraction_dict:\n",
    "            text = text.replace(word, contraction_dict[word])\n",
    "    return text\n",
    "\n",
    "\n",
    "data[\"title\"] = data[\"title\"].apply(contraction_replacer)\n",
    "data[\"content\"] = data[\"content\"].apply(contraction_replacer)\n",
    "\n",
    "# Remove punctuation and numbers\n",
    "# Find more punctuation in text and add\n",
    "\n",
    "numbers = '0123456789'\n",
    "\n",
    "\n",
    "def punctuation_numbers_remover(text):\n",
    "    for number in numbers:\n",
    "        text = text.replace(number, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "data[\"title\"] = data[\"title\"].apply(punctuation_numbers_remover)\n",
    "data[\"content\"] = data[\"content\"].apply(punctuation_numbers_remover)\n",
    "\n",
    "data.head()\n",
    "\n",
    "# plot the distribution of the lengths of the sequences\n",
    "content_lengths = [len(content.split()) for content in data[\"content\"]]\n",
    "\n",
    "\n",
    "# remove the outliers\n",
    "MAX_CONTENT_LENGTH = 1000\n",
    "MAX_TITLE_LENGTH = 10\n",
    "# get the indices of the documents that have more than 1000 words\n",
    "outliers = [idx for idx, length in enumerate(\n",
    "    content_lengths) if length > MAX_CONTENT_LENGTH]\n",
    "\n",
    "# remove the outliers from the data\n",
    "data = data.drop(outliers, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\giris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\giris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 2203/2204\r"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "\n",
    "predictions = []\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "\n",
    "for i in range (data[\"content\"].shape[0]):\n",
    "    print(f'Article {i}/{data[\"content\"].shape[0]}', end='\\r')\n",
    "    text = data[\"content\"][i]\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize sentences into words, remove punctuation, and convert to lower case\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words_list = [tokenizer.tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_words = [[word for word in words if word not in stop_words]\n",
    "                      for words in words_list]\n",
    "\n",
    "    # Combine preprocessed words into sentences again\n",
    "    preprocessed_sentences = [' '.join(words) for words in filtered_words]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer and transform sentences into TF-IDF vectors\n",
    "    tfidf_vectors = vectorizer.fit_transform(preprocessed_sentences)\n",
    "\n",
    "    # Create a graph to represent the sentences and their connections\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Add nodes to the graph (each sentence is a node)\n",
    "    graph.add_nodes_from(sentences)\n",
    "\n",
    "    # Calculate cosine similarity between sentences and create edges between them based on similarity\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                similarity = tfidf_vectors[i].dot(tfidf_vectors[j].T).toarray()[\n",
    "                    0][0]  # Compute cosine similarity\n",
    "                graph.add_edge(sentences[i], sentences[j], weight=similarity)\n",
    "\n",
    "    # Apply the PageRank algorithm to rank the sentences\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    # Sort the sentences based on their scores\n",
    "    ranked_sentences = sorted(((scores[sentence], sentence)\n",
    "                              for sentence in sentences), reverse=True)\n",
    "\n",
    "    # Number of sentences to include in the summary\n",
    "    num_sentences = 1  # Change this as needed\n",
    "\n",
    "    # Extract top-ranked sentences to create the summary\n",
    "    summary_sentences = sorted(ranked_sentences[:num_sentences])\n",
    "\n",
    "    # Generate the final summary\n",
    "    summary = \" \".join(sentence[1] for sentence in summary_sentences)\n",
    "\n",
    "    # print(\"Original Text:\\n\", text)\n",
    "    # print(\"\\nSummary:\\n\", summary)\n",
    "    predictions.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  0.13455555123929014\n",
      "ROUGE-L:  0.11793104975838294\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "# Evaluation\n",
    "\n",
    "\n",
    "rouge1 = 0.0\n",
    "rougeL = 0.0\n",
    "\n",
    "def calculate_rouge(reference, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return scores\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    sc = calculate_rouge(data[\"title\"][i], predictions[i])\n",
    "    rouge1 += sc[\"rouge1\"].fmeasure\n",
    "    rougeL += sc[\"rougeL\"].fmeasure\n",
    "\n",
    "rouge1 /= len(predictions)\n",
    "rougeL /= len(predictions)\n",
    "\n",
    "print(\"ROUGE-1: \", rouge1)\n",
    "print(\"ROUGE-L: \", rougeL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
